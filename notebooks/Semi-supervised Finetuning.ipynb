{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be18d87e",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This notebook guides through running different Semi-supervised fine-tuning pipelines implemented in this repo. This notebook uses custom `CSV` files as datasets for fine-tuning BERT-like models.<br/>\n",
    "\n",
    "I have placed sample dataset files for getting familiarized with the input format of the dataset files. Please ensure the dataset each dataset file has the following columns:\n",
    "1. `smiles` -> Input molecular SMILES.\n",
    "2. `y` -> corresponds to the labels. For unlabelled samples used in Semi-supervised models set y values to `-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95ae8e",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Setup the environment by creating a virtual environment via conda using the following command. <br/>\n",
    "`conda create -n drug_discovery_v1 python==3.8.13` <br/>\n",
    "\n",
    "Install the required dependencies.<br/>\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab9f05",
   "metadata": {},
   "source": [
    "## 1. Supervised Fine-tuning using custom files (MLM/MTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this before running the cell below if and change the name of output file as per requirement\n",
    "#!touch eval_result_supervised.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295379a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup parameters/hyperparamters for training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Data related configuration\n",
    "\"\"\"\n",
    "datasets = ['custom'] # set always to custom when reading data from CSV\n",
    "#path of the folder in which this notebook is located, can be obtained by running `pwd` command on linux\n",
    "folder_path=\"/Users/shahrukh/Desktop/moleculenet-bert-ssl/notebooks\" \n",
    "train_path=f'{folder_path}/data/train.csv' # replace this with full path to train split of your input data\n",
    "valid_path=f'{folder_path}/data/val.csv' # replace this with full path to validation split of your input data\n",
    "test_path=f'{folder_path}/data/test.csv' # replace this with full path to test split of your input data\n",
    "out_file = f'{folder_path}/eval_result_supervised_augment.csv' # full path to output file, ensure this file already exists\n",
    "SAMPLES_PER_CLASS = [200] # set to maximum number of samples in a class\n",
    "N_AUGMENT = [2] # number of augmentations on each input sample\n",
    "\n",
    "\"\"\"\n",
    "Training related configuration\n",
    "\"\"\"\n",
    "N_TRIALS = 20 # number of times you want to repeat the training to obtain standard error later\n",
    "EPOCHS = 20 # number of training epochs per trial\n",
    "model_name_or_path= \"shahrukhx01/smole-bert\" #name of the model from huggingface model hub or path from file system\n",
    "DO_SEMI_SUPERVISED_TRAINING=0 # SET this to `0` when you want to do supervised training \n",
    "\n",
    "## run the experiment here\n",
    "for dataset in datasets:\n",
    "    for SAMPLE in SAMPLES_PER_CLASS:\n",
    "        for n_augment in N_AUGMENT:\n",
    "            for i in tqdm(range(N_TRIALS)):\n",
    "                !python ../pseudo_label/main.py --dataset-name={dataset} --epochs={EPOCHS} \\\n",
    "                --batch-size=16 --model-name-or-path={model_name_or_path} --samples-per-class={SAMPLE} \\\n",
    "                --eval-after={EPOCHS} --train-log=0 --train-ssl={DO_SEMI_SUPERVISED_TRAINING} --out-file={out_file} \\\n",
    "                --n-augment={n_augment} --train-path={train_path} --val-path={valid_path} --test-path={test_path}\n",
    "                !cat {out_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a592d4",
   "metadata": {},
   "source": [
    "## 2. Semi-supervised Pseudo-label-based Fine-tuning using custom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this before running the cell below if and change the name of output file as per requirement\n",
    "#!touch eval_result_pseudo_label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup parameters/hyperparamters for training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Data related configuration\n",
    "\"\"\"\n",
    "datasets = ['custom'] # set always to custom when reading data from CSV\n",
    "#path of the folder in which this notebook is located, can be obtained by running `pwd` command on linux\n",
    "folder_path=\"/Users/shahrukh/Desktop/moleculenet-bert-ssl/notebooks\" \n",
    "train_path=f'{folder_path}/data/train_semi_supervised.csv' # replace this with full path to train split of your input data\n",
    "valid_path=f'{folder_path}/data/val.csv' # replace this with full path to validation split of your input data\n",
    "test_path=f'{folder_path}/data/test.csv' # replace this with full path to test split of your input data\n",
    "out_file = f'{folder_path}/eval_result_pseudo_label.csv' # full path to output file, ensure this file already exists\n",
    "SAMPLES_PER_CLASS = [200] # set to maximum number of samples in a class\n",
    "N_AUGMENT = [2] # number of augmentations on each input sample\n",
    "\n",
    "\"\"\"\n",
    "Training related configuration\n",
    "\"\"\"\n",
    "N_TRIALS = 20 # number of times you want to repeat the training to obtain standard error later\n",
    "EPOCHS = 20 # number of training epochs per trial\n",
    "model_name_or_path= \"shahrukhx01/smole-bert\" #name of the model from huggingface model hub or path from file system\n",
    "DO_SEMI_SUPERVISED_TRAINING=1 # SET this to `1` when you want to do Semi-supervised training \n",
    "\n",
    "## run the experiment here\n",
    "for dataset in datasets:\n",
    "    for SAMPLE in SAMPLES_PER_CLASS:\n",
    "        for n_augment in N_AUGMENT:\n",
    "            for i in tqdm(range(N_TRIALS)):\n",
    "                !python ../pseudo_label/main.py --dataset-name={dataset} --epochs={EPOCHS} \\\n",
    "                --batch-size=16 --model-name-or-path={model_name_or_path} --samples-per-class={SAMPLE} \\\n",
    "                --eval-after={EPOCHS} --train-log=0 --train-ssl={DO_SEMI_SUPERVISED_TRAINING} --out-file={out_file} \\\n",
    "                --n-augment={n_augment} --train-path={train_path} --val-path={valid_path} --test-path={test_path}\n",
    "                !cat {out_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd003143",
   "metadata": {},
   "source": [
    "## 3. Semi-supervised Co-training-based Fine-tuning using custom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e97413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this before running the cell below if and change the name of output file as per requirement\n",
    "#!touch eval_result_co_training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup parameters/hyperparamters for training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Data related configuration\n",
    "\"\"\"\n",
    "datasets = ['custom'] # set always to custom when reading data from CSV\n",
    "#path of the folder in which this notebook is located, can be obtained by running `pwd` command on linux\n",
    "folder_path=\"/Users/shahrukh/Desktop/moleculenet-bert-ssl/notebooks\" \n",
    "train_path=f'{folder_path}/data/train_semi_supervised.csv' # replace this with full path to train split of your input data\n",
    "valid_path=f'{folder_path}/data/val.csv' # replace this with full path to validation split of your input data\n",
    "test_path=f'{folder_path}/data/test.csv' # replace this with full path to test split of your input data\n",
    "out_file = f'{folder_path}/eval_result_co_training.csv' # full path to output file, ensure this file already exists\n",
    "SAMPLES_PER_CLASS = [200] # set to maximum number of samples in a class\n",
    "\n",
    "\"\"\"\n",
    "Training related configuration\n",
    "\"\"\"\n",
    "N_TRIALS = 20 # number of times you want to repeat the training to obtain standard error later\n",
    "EPOCHS = 20 # number of training epochs per trial\n",
    "model_name_or_path= \"shahrukhx01/smole-bert\" #name of the model from huggingface model hub or path from file system\n",
    "posterior_threshold = 0.9\n",
    "DO_SEMI_SUPERVISED_TRAINING=1 # SET this to `1` when you want to do Semi-supervised training \n",
    "\n",
    "## run the experiment here\n",
    "for dataset in datasets:\n",
    "    for SAMPLE in SAMPLES_PER_CLASS:\n",
    "        for i in tqdm(range(N_TRIALS)):\n",
    "            !python ../co_training/main.py --dataset-name={dataset} --epochs={EPOCHS} \\\n",
    "            --batch-size=16 --model-name-or-path={model_name_or_path} --samples-per-class={SAMPLE} \\\n",
    "            --eval-after={EPOCHS} --train-log=0 --train-ssl={DO_SEMI_SUPERVISED_TRAINING} --out-file={out_file} \\\n",
    "            --train-path={train_path} --val-path={valid_path} --test-path={test_path} --posterior-threshold={posterior_threshold}\n",
    "            !cat {out_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
